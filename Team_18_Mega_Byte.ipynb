{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Team_18_Mega_Byte.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KaO-Uuk-98ha"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanielKozickiUofC/ENEL645/blob/main/Team_18_Mega_Byte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRn_FN_0GqtQ"
      },
      "source": [
        "# **This is a compilation of the code brough in from other files into 1. The sections will have a header for where it came from**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvQqgGJHgBB"
      },
      "source": [
        "# **Config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQw58tolS8lf",
        "outputId": "71c4e5c9-6bd1-418b-b061-09711e7816f9"
      },
      "source": [
        "!pip install datetime"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datetime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/22/a5297f3a1f92468cc737f8ce7ba6e5f245fcfafeae810ba37bd1039ea01c/DateTime-4.3-py2.py3-none-any.whl (60kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 12.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 13.6MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 40kB 7.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 51kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from datetime) (2018.9)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/57/8a68360d697cf9159cba5ee35f2d25bdcda33883e8b5a997714a191a0b11/zope.interface-5.3.0-cp37-cp37m-manylinux2010_x86_64.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface->datetime) (54.2.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHVfiVLTEzqN"
      },
      "source": [
        "# This is a config file for the project, it will contain all configurable variables in one place\n",
        "# Once development of the code is complete, this should be the only file that needs to be edited.\n",
        "import datetime\n",
        "# Set to True if you want to print some information about the model and\n",
        "# view sample images from the dataset\n",
        "VISUALIZATION = False\n",
        "\n",
        "# Set to True if you want to see the histogram for the dataset\n",
        "HISTOGRAM = False\n",
        "\n",
        "# Set to True to apply data Augmentation to the dataset before training\n",
        "# Options:\n",
        "DATA_AUGMENTATION = False\n",
        "\n",
        "# Select the Model you want to train\n",
        "# Options:\n",
        "MODEL = 5\n",
        "\n",
        "# Specify the number of epochs for training\n",
        "EPOCHS = 1\n",
        "\n",
        "# Specify the batch size for pre-processing and training\n",
        "# Ex: 32, 64, 128\n",
        "BATCH = 32\n",
        "\n",
        "# Below are the Variables dependant on the Model selected\n",
        "if MODEL == 1 and DATA_AUGMENTATION is False:\n",
        "    # Using Model 1 without data augmentation\n",
        "    MODEL_ID = 1\n",
        "    LOG_DIR = \"./tmp/logs/model1\"\n",
        "    MODEL_NAME = \"cnn_model_1\"\n",
        "\n",
        "elif MODEL == 1 and DATA_AUGMENTATION is not False:\n",
        "    # Using Model 1 with data augmentation\n",
        "    MODEL_ID = 2\n",
        "    LOG_DIR = \"./tmp/logs/model1_data_aug\"\n",
        "    MODEL_NAME = \"cnn_model_1_data_aug\"\n",
        "\n",
        "elif MODEL == 2 and DATA_AUGMENTATION is False:\n",
        "    # Using Model 2 without data augmentation\n",
        "    MODEL_ID = 3\n",
        "    LOG_DIR = \"./tmp/logs/model2\"\n",
        "    MODEL_NAME = \"cnn_model_2\"\n",
        "\n",
        "elif MODEL == 2 and DATA_AUGMENTATION is not False:\n",
        "    # Using Model 2 with data augmentation\n",
        "    MODEL_ID = 4\n",
        "    LOG_DIR = \"./tmp/logs/model2_data_aug\"\n",
        "    MODEL_NAME = \"cnn_model_2_data_aug\"\n",
        "\n",
        "elif MODEL == 5 and DATA_AUGMENTATION is False:\n",
        "    # Using Model 2 (Transfer Learning VGG16) without data augmentation\n",
        "    MODEL_ID = 5\n",
        "    LOG_DIR = \"./tmp/logs/model_TL\"\n",
        "    MODEL_NAME = \"TL_model_VGG16\"\n",
        "\n",
        "else:\n",
        "    # Error Did not Select a Model\n",
        "    print(\"ERROR: Value entered for MODEL is incorrect. Received: \", MODEL, \"\\n EXPECTING: 1 or 2\")\n",
        "    exit(500)\n",
        "\n",
        "LOG_FILE = LOG_DIR + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GTW0GOKRBl"
      },
      "source": [
        "# **Utils**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3AJWc0rKPu7"
      },
      "source": [
        "# This file contains common functions for data processing independent of the dataset and model used.\n",
        "# Not all of these functions are required to be used in the project\n",
        "import tensorflow as tf\n",
        "#import config\n",
        "\n",
        "# Loading Tensorboard Logging dir and file\n",
        "log_dir = LOG_DIR\n",
        "log_file = LOG_FILE\n",
        "model_name = MODEL_NAME\n",
        "\n",
        "# ------- THE FUNCTIONS BELOW ARE USED IN THIS PROJECT ------- #\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "data_augmentation_flip_rotate = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)])\n",
        "\n",
        "data_scaling_resizing = tf.keras.Sequential([\n",
        "    tf.keras.layers.experimental.preprocessing.Resizing(512, 512),\n",
        "    tf.keras.layers.experimental.preprocessing.Rescaling(1. / 255)])\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
        "\n",
        "monitor_func = tf.keras.callbacks.ModelCheckpoint(model_name, monitor='val_loss',\n",
        "                                                  verbose=0, save_best_only=True,\n",
        "                                                  save_weights_only=True, mode='min')\n",
        "# Learning rate schedule\n",
        "def scheduler(epoch, lr):\n",
        "    if epoch % 10 == 0:\n",
        "        lr = lr / 2\n",
        "    return lr\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaO-Uuk-98ha"
      },
      "source": [
        "### The functions below will not be used but are here kept anyway"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDU8xor_945B"
      },
      "source": [
        "# ------- THE FUNCTIONS BELOW WILL NoT be USED IN THIS PROJECT ------- #\n",
        "# ------- They Did not work with our dataset\n",
        "\n",
        "\n",
        "# Shuffle indexes of given dataset and labels\n",
        "def shuffle_indexes(X, Y):\n",
        "    import numpy as np\n",
        "    indexes = np.arange(X.shape[0], dtype=int)\n",
        "    np.random.shuffle(indexes)\n",
        "    X_new = X[indexes]\n",
        "    Y_new = Y[indexes]\n",
        "    return X_new, Y_new\n",
        "\n",
        "\n",
        "# Splitting the given dataset (dataset_X, dataset_Y) into two portions\n",
        "# dataset_X is the data values and dataset_Y are the corresponding labels\n",
        "# (X_LG, Y_LG) will have the first {percent*100}% of the dataset and\n",
        "# (X_SM, Y_SM) will have the last {1 - percent}*100% of the dataset\n",
        "def split_dataset(dataset_X, dataset_Y, percent):\n",
        "    # Calculate splitting index\n",
        "    nsplit = int(percent * dataset_X.shape[0])\n",
        "\n",
        "    # split dataset into\n",
        "    X_LG = dataset_X[:nsplit]\n",
        "    Y_LG = dataset_Y[:nsplit]\n",
        "    X_SM = dataset_X[nsplit:]\n",
        "    Y_SM = dataset_Y[nsplit:]\n",
        "    return X_LG, Y_LG, X_SM, Y_SM\n",
        "\n",
        "\n",
        "# Returns One Hot Encoding for given train, validate and test dataset labels or None\n",
        "def one_hot_encoding(train=None, validate=None, test=None):\n",
        "    train_oh = None\n",
        "    validate_oh = None\n",
        "    test_oh = None\n",
        "    if train is not None:\n",
        "        train_oh = tf.keras.utils.to_categorical(train)\n",
        "    if validate is not None:\n",
        "        validate_oh = tf.keras.utils.to_categorical(validate)\n",
        "    if test is not None:\n",
        "        test_oh = tf.keras.utils.to_categorical(test)\n",
        "    return train_oh, validate_oh, test_oh\n",
        "\n",
        "\n",
        "# returns normalized dataset values\n",
        "# norm_type = 0 -> min-max; norm_type = 1 -> standardization\n",
        "def normalise_data(train, val, test, norm_type=0):\n",
        "    if norm_type == 0:\n",
        "        X_train = train / 255\n",
        "        X_val = val / 255\n",
        "        X_test = test / 255\n",
        "    else:\n",
        "        train_mean, train_std = train.mean(), train.std()\n",
        "        X_train = (train - train_mean) / train_std\n",
        "        X_val = (val - train_mean) / train_std\n",
        "        X_test = (test - train_mean) / train_std\n",
        "    return X_train, X_val, X_test\n",
        "\n",
        "\n",
        "# Resize Images from TFDS Dataset\n",
        "def resize_dataset(img, label):\n",
        "    img = tf.image.resize(img, (512, 512))\n",
        "    return img, label\n",
        "\n",
        "\n",
        "# Extract Images and Labels in Dataset\n",
        "def feature_extraction(ds):\n",
        "    img = []\n",
        "    lbl = []\n",
        "    for i in ds:\n",
        "        img.append(i[0])\n",
        "        lbl.append(i[1])\n",
        "    return img, lbl\n",
        "\n",
        "\n",
        "# Normalizes Images in Dataset using min-max method\n",
        "def dataset_normalization_min_max(img, lbl):\n",
        "    img = img / 255\n",
        "    return img, lbl\n",
        "\n",
        "\n",
        "# Normalizes Images in Dataset using Standardization method\n",
        "def dataset_normalization_std(img, lbl):\n",
        "    mean, std = img.mean(), img.std()\n",
        "    img = (img - mean) / std\n",
        "    return img, lbl\n",
        "\n",
        "\n",
        "# Augment Images in Dataset using random flip method\n",
        "def dataset_augmentation_flip(img, lbl):\n",
        "    return tf.image.random_flip_left_right(img), lbl\n",
        "\n",
        "\n",
        "# Augment Images in Dataset using random contrast method\n",
        "def dataset_augmentation_contrast(img, lbl):\n",
        "    return tf.image.random_contrast(img, lower=0.0, upper=1.0), lbl"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3_bYv-uHs1v"
      },
      "source": [
        "# **CNN Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fw3KfigTOWG8"
      },
      "source": [
        "Note that since transfer learning has a coarse fit and fine tune it is split up into seperate cells. These are defined below under training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpn-kHBfFKvB"
      },
      "source": [
        "import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "#import config\n",
        "\n",
        "model_name = MODEL_NAME\n",
        "\n",
        "\n",
        "# Different CNN are defined below\n",
        "# After determining which architecture is better (model 1 or model 2)\n",
        "# TODO apply different dropouts to the best one\n",
        "\n",
        "def model_1(num_class, k=128):\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), input_shape=(512, 512, 3), data_format=\"channels_last\", padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(k, activation='relu'),\n",
        "        tf.keras.layers.Dense(num_class, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# CNN From Assignment 2\n",
        "def model_2(k=101):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(48, (3, 3), input_shape=(512, 512, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(48, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(96, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.Conv2D(96, (3, 3), padding='same', activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(k, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "def compile_model(model,  lr=1e-4, optim=0):\n",
        "    if optim == 1:\n",
        "        optimizer_cnn = tf.keras.optimizers.SGD(learning_rate=lr)\n",
        "    else:\n",
        "        optimizer_cnn = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=optimizer_cnn, loss='categorical_crossentropy',\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBhh2OQnH6Z7"
      },
      "source": [
        "# **Dataset Prep**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kwxJg4oJ2fv"
      },
      "source": [
        "load dataset and get things ready"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct73LWYI8XDt"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "#import utils\n",
        "#import CNN\n",
        "#import config\n",
        "\n",
        "# Load Variables from Configuration File\n",
        "VISUALIZE_IMG = VISUALIZATION\n",
        "HISTOGRAM = HISTOGRAM\n",
        "norm_type = \"min-max\"\n",
        "data_augment = DATA_AUGMENTATION\n",
        "MODEL = MODEL_ID\n",
        "EPOCHS = EPOCHS\n",
        "LOG_DIR = LOG_DIR\n",
        "LOG_FILE = LOG_FILE\n",
        "MODEL_NAME = MODEL_NAME\n",
        "batch = BATCH\n",
        "num_classes = 101"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2CHYZcY8bgq"
      },
      "source": [
        "# Get GPU Working with CuDNN\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Currently, memory growth needs to be the same across GPUs\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "        # Memory growth must be set before GPUs have been initialized\n",
        "        print(e)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwO7tPu0kEMm"
      },
      "source": [
        "## **Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypobUJI1JriX",
        "outputId": "41a90dc7-b674-4a9f-9c80-fe5a366d1508"
      },
      "source": [
        "# ---------- LOADING Dataset ---------- #\n",
        "# Loading Food 101 train validation and test datasets with shuffled indexes  'validation',\n",
        "(food101_ds_train, food101_ds_val, food101_ds_test), metadata = tfds.load('food101',\n",
        "                                                                          split=['train[:86%]', 'validation', 'train[-14%:]'],\n",
        "                                                                          shuffle_files=False, as_supervised=True, with_info=True) #swi\n",
        "print(\"Split Keys: \", list(metadata.splits.keys()))\n",
        "print(\"info.features: \", metadata.features)\n",
        "print(\"train type before: \", type(food101_ds_train))\n",
        "print(\"Num of Classes: \", metadata.features[\"label\"].num_classes)\n",
        "print(\"Lengths: \", len(food101_ds_train), len(food101_ds_val), len(food101_ds_test))\n",
        "exit(0)\n",
        "assert isinstance(food101_ds_train, tf.data.Dataset), \"Training dataset is not a TF Dataset\"\n",
        "assert isinstance(food101_ds_val, tf.data.Dataset), \"Validation dataset is not a TF Dataset\"\n",
        "assert isinstance(food101_ds_test, tf.data.Dataset), \"Test dataset is not a TF Dataset\"\n",
        "\n",
        "get_label_name = metadata.features['label'].int2str\n",
        "num_training = tf.data.experimental.cardinality(food101_ds_train).numpy()\n",
        "print(\"Num training images: \", str(num_training))\n",
        "print(\"Num Val images: \", str(tf.data.experimental.cardinality(food101_ds_val).numpy()))\n",
        "print(\"Num test images: \", str(tf.data.experimental.cardinality(food101_ds_test).numpy()))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Split Keys:  ['train', 'validation']\n",
            "info.features:  FeaturesDict({\n",
            "    'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "    'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=101),\n",
            "})\n",
            "train type before:  <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n",
            "Num of Classes:  101\n",
            "Lengths:  65145 25250 10605\n",
            "Num training images:  65145\n",
            "Num Val images:  25250\n",
            "Num test images:  10605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "U1t5-3tqJ3hF",
        "outputId": "5f2172d7-18b5-4217-a063-13cdcb0662dd"
      },
      "source": [
        "# ---------- Pre-Processing Dataset ---------- #\n",
        "# Resizing training and validation datasets to be (512, 512, 3)\n",
        "# Applying One-Hot encoding to Labels\n",
        "# DO NOT APPLY THIS TO THE TESTING SET\n",
        "print(\"train type before: \", type(food101_ds_train))\n",
        "food101_ds_train = food101_ds_train.map(lambda im_t, l_t: (data_scaling_resizing(im_t, training=True), l_t))\n",
        "food101_ds_train = food101_ds_train.map(lambda im_t, l_t: (im_t, tf.one_hot(l_t,depth=101)))\n",
        "food101_ds_val = food101_ds_val.map(lambda im_v, l_v: (data_scaling_resizing(im_v, training=True), l_v))\n",
        "food101_ds_val = food101_ds_val.map(lambda im_v, l_v: (im_v, tf.one_hot(l_v,depth=101)))\n",
        "food101_ds_train = food101_ds_train.batch(batch)\n",
        "food101_ds_val = food101_ds_val.batch(batch)\n",
        "print(\"train type after: \", type(food101_ds_train))\n",
        "\n",
        "i = 0\n",
        "for img, lbl in food101_ds_train:\n",
        "    i += 1\n",
        "    print(\"Train take one: \", img.shape, \" lbl shape: \", lbl.shape, \" lbl class: \", type(lbl) )\n",
        "    if i > 4:\n",
        "        break"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9fd72463ec57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Applying One-Hot encoding to Labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# DO NOT APPLY THIS TO THE TESTING SET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train type before: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood101_ds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mfood101_ds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfood101_ds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mim_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_scaling_resizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfood101_ds_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfood101_ds_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mim_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mim_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'food101_ds_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HF0aW-Q-wps"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ifpMKcgDfCY",
        "outputId": "66b12343-5395-4b52-994f-ed69bcc02b15"
      },
      "source": [
        "# ---------- Training Models ---------- #\n",
        "print(\"Training Model \", MODEL, \"  Norm Type: \", norm_type, \"  Data Aug: \", data_augment)\n",
        "k = 128\n",
        "optimizer = 0\n",
        "learning_rate = 1e-4\n",
        "if MODEL == 1:\n",
        "    model1 = model_1(num_classes, k)  \n",
        "    model1 = compile_model(model1, learning_rate, optimizer)\n",
        "    history1 = model1.fit(food101_ds_train, validation_data=food101_ds_val,\n",
        "                          epochs=EPOCHS, verbose=1, batch_size=batch,\n",
        "                          callbacks=[early_stop, monitor_func,\n",
        "                                     lr_schedule, tensorboard_callback])\n",
        "elif MODEL == 2:\n",
        "    food101_ds_train = food101_ds_train.map(lambda im_t, l_t: (data_augmentation_flip_rotate(im_t, training=True), l_t))\n",
        "    food101_ds_val = food101_ds_val.map(lambda im_v, l_v: (data_augmentation_flip_rotate(im_v, training=True), l_v))\n",
        "\n",
        "    model2 = model_1(num_classes, k)\n",
        "    model2 = compile_model(model2, learning_rate, optimizer)\n",
        "    history2 = model2.fit(food101_ds_train, validation_data=food101_ds_val,\n",
        "                          epochs=EPOCHS, verbose=1,\n",
        "                          callbacks=[early_stop, monitor_func,\n",
        "                                     lr_schedule, tensorboard_callback])\n",
        "elif MODEL == 3:\n",
        "    model3 = model_2(num_classes)\n",
        "    model3 = compile_model(model3, learning_rate, optimizer)\n",
        "    history3 = model3.fit(food101_ds_train, validation_data=food101_ds_val,\n",
        "                          epochs=EPOCHS, verbose=1,\n",
        "                          callbacks=[early_stop, monitor_func,\n",
        "                                     lr_schedule, tensorboard_callback])\n",
        "elif MODEL == 4:\n",
        "    food101_ds_train = food101_ds_train.map(lambda im_t, l_t: (data_augmentation_flip_rotate(im_t, training=True), l_t))\n",
        "    food101_ds_val = food101_ds_val.map(lambda im_v, l_v: (data_augmentation_flip_rotate(im_v, training=True), l_v))\n",
        "\n",
        "    model4 = model_2(num_classes)\n",
        "    model4 = compile_model(model4, learning_rate, optimizer)\n",
        "    history4 = model4.fit(food101_ds_train, validation_data=food101_ds_val,\n",
        "                          epochs=EPOCHS, verbose=1,\n",
        "                          callbacks=[early_stop, monitor_func,\n",
        "                                     lr_schedule, tensorboard_callback])\n",
        "# Transfer Learning\n",
        "elif MODEL ==5:\n",
        "        print(\"Tranfer learning selected. Proceed to next cell\") # seperate from other models due to coarse and mine model training needed\n",
        "\n",
        "else:\n",
        "  print(\"error! no valid model selected\")\n",
        "\n",
        "# TensorBoard Logging ------- Optional\n",
        "writer = tf.summary.create_file_writer(LOG_DIR)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Model  5   Norm Type:  min-max   Data Aug:  False\n",
            "Tranfer learning selected. Proceed to next cell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oynh7Bv3MLri"
      },
      "source": [
        "## **Transfer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcf-1eglLAWm",
        "outputId": "e218e3a5-f806-4cec-8e36-2216f75396f6"
      },
      "source": [
        "if MODEL == 5: #Transfer Learning model setup (VGG16)\n",
        "  img_height = 512\n",
        "  img_width = 512\n",
        "\n",
        "  base_VGG16 = tf.keras.applications.VGG16( # Base model must have unique name as multiple models are in play\n",
        "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=(img_height, img_width, 3), # Recieves input shape as specified\n",
        "    include_top=False) # removes prediction layer so we can add our own\n",
        "\n",
        "  base_VGG16.trainable = False # freezes model weights so we can train a new prediction layer\n",
        "  input_image = tf.keras.Input(shape=(img_height, img_width, 3)) # define input size for model\n",
        "  x1 = base_VGG16(input_image, training=False) # this layer is the VGG16 model we brought in without predicting layer\n",
        "  x2 = tf.keras.layers.Flatten()(x1) #flattens VGG16 output for dense prediction layer\n",
        "  out = tf.keras.layers.Dense(num_classes,activation = 'softmax')(x2)\n",
        "  model_VGG16 = tf.keras.Model(inputs = input_image, outputs =out) # Model must have unique name as multiple models are in play (VGG16, CNN)\n",
        "\n",
        "  print(model_VGG16.summary())\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 16, 16, 512)       14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 101)               13238373  \n",
            "=================================================================\n",
            "Total params: 27,953,061\n",
            "Trainable params: 13,238,373\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjIURyr2LuIK",
        "outputId": "ce733a9c-8b9b-466c-f150-2ad22a8744f4"
      },
      "source": [
        "if MODEL == 5: #load in weights if training is broken into sessions due to long training times\n",
        "  print(\"Transfer learning selected\")\n",
        "  #from numpy import loadtxt\n",
        "  #from tensorflow.keras.models import load_model\n",
        "\n",
        "  #model_VGG16 = load_model(\"VGG16_Team_18_Final_Project__pft_apr12.h5\") #change date, pft = pre fine tuning\n",
        "  ##check to make sure it worked\n",
        "  #model_VGG16.summary()\n",
        "\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transfer learning selected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "OBQGm-yoMzPH",
        "outputId": "ceda0b51-b386-46d6-930e-909fddf8fabd"
      },
      "source": [
        "if MODEL == 5: #compile and fit model (Coarse fit)\n",
        "  model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "  model_VGG16.fit(food101_ds_train,epochs = EPOCHS, \\\n",
        "        verbose = 1, callbacks= [early_stop, monitor_func, lr_schedule],validation_data= food101_ds_val) # check that data names are correct\n",
        "\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-63c16e6c0db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mmodel_VGG16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood101_ds_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfood101_ds_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# check that data names are correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'food101_ds_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfUtDkX2M_cP",
        "outputId": "45c76fe3-3f3e-489d-8dfc-0b2d5f05039f"
      },
      "source": [
        "if MODEL == 5: #compile and fit model\n",
        "  print(\"Transfer learning selected\")\n",
        "  ##save VGG16 model weights - to be used if training is broken into sessions\n",
        "  #model_VGG16.save(\"VGG16_Team_18_Final_Project__pft_apr12.h5.h5\") #ensure date correct\n",
        "\n",
        "  #print(\"saved model weights\")\n",
        "\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transfer learning selected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V7XHGCiNbno"
      },
      "source": [
        "### Transfer Learning Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "v0o9WbKBNRtu",
        "outputId": "f570d4d7-cd8a-4079-e3d9-ee46a362daf6"
      },
      "source": [
        "if MODEL == 5: #Fine tuning model\n",
        "  model_VGG16.trainable = True\n",
        "  #model_VGG16 = load_model('VGG16_Team_18_Final_Project_pft_apr12.h5') #change date -to be used if training is broken into sessions or just training fine tunin, pft = pre fine tuning\n",
        "  model_VGG16.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-9), # Very low learning rate to fine tune\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "  print(model_VGG16.summary())\n",
        "  model_VGG16.fit(food101_ds_train, epochs = EPOCHS, \\\n",
        "            verbose = 1, callbacks= [early_stop, monitor_func, lr_schedule],validation_data=food101_ds_val)\n",
        "\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 16, 16, 512)       14714688  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 101)               13238373  \n",
            "=================================================================\n",
            "Total params: 27,953,061\n",
            "Trainable params: 27,953,061\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-9eb0b3be0c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                metrics=['accuracy'])\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_VGG16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mmodel_VGG16\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfood101_ds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfood101_ds_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'food101_ds_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMmpmdIFNgkd",
        "outputId": "e0446b77-b28b-4f39-9646-bb3c37249eb0"
      },
      "source": [
        "if MODEL == 5: #Save Model weights after fine tuning\n",
        "  print(\"Transfer learning selected\")\n",
        "  #model_VGG16.save('VGG16_Team_18_Final_Project_aft_apr12.h5') #saves weigths, aft = after fine tuning\n",
        "\n",
        "  #print(\"saved model weights\")\n",
        "\n",
        "else:\n",
        "  print(\"Transfer learning not selected\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transfer learning selected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaQKJIguOnhV"
      },
      "source": [
        "# **Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgXHKXRJ9aMV"
      },
      "source": [
        "# ---------- Visualizing Dataset ---------- #\n",
        "if VISUALIZE_IMG is True:\n",
        "    # Method 1 of splitting Dataset to Images and Labels\n",
        "    # train_X, train_Y = tuple(zip(*food101_ds_train))\n",
        "    # val_X, val_Y = tuple(zip(*food101_ds_val))\n",
        "    # test_X, test_Y = tuple(zip(*food101_ds_test))\n",
        "\n",
        "    # Method 2 of Splitting ataset to Images and Labels\n",
        "    train_X, train_Y = feature_extraction(food101_ds_train)\n",
        "    val_X, val_Y = feature_extraction(food101_ds_val)\n",
        "    test_X, test_Y = feature_extraction(food101_ds_test)\n",
        "\n",
        "    # Printing some information about the Dataset\n",
        "    print(\"info.features: \", metadata.features)\n",
        "    print(\"Split Keys: \", list(metadata.splits.keys()))\n",
        "    # print(\"Num of Training examples 1%: \", info.splits['validation[1%:]'].num_examples)\n",
        "    print(\"Num of Classes: \", metadata.features[\"label\"].num_classes)\n",
        "    # print(\"Classes: \", info.features[\"label\"].names)\n",
        "\n",
        "    # Displaying Figure of samples from Dataset\n",
        "    fig2 = tfds.show_examples(food101_ds_train, metadata, rows=5)\n",
        "\n",
        "    # Displaying Figure of samples from Dataset using PyPlot\n",
        "    i = 0\n",
        "    plt.figure(1)\n",
        "    for img, lbl in food101_ds_train:\n",
        "        i = i + 1\n",
        "        print(\"Train take one: \", img.shape, int(lbl))\n",
        "        ax = plt.subplot(2, 2, i)\n",
        "        plt.imshow(img.numpy().astype(\"uint8\"))\n",
        "        ax.set_title(get_label_name(lbl))\n",
        "        plt.axis(\"off\")\n",
        "        if i >= 4:\n",
        "            break\n",
        "\n",
        "    # Displaying Figure of samples from Dataset using PyPlot\n",
        "    # and using the Images and Labels dataset instead of the combined one (for comparison)\n",
        "    i = 0\n",
        "    plt.figure(2)\n",
        "    for item in train_X:\n",
        "        i = i + 1\n",
        "        print(\"Train img: \", item.shape)\n",
        "        ax = plt.subplot(2, 2, i)\n",
        "        plt.imshow(item.numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "        if i >= 4:\n",
        "            break\n",
        "\n",
        "    i = 0\n",
        "    for item in train_Y:\n",
        "        i = i + 1\n",
        "        print(\"Train lbl: \", int(item), \"  name: \", get_label_name(item))\n",
        "        if i >= 4:\n",
        "            break\n",
        "    plt.show()\n",
        "\n",
        "    # Displaying the Histogram\n",
        "    if HISTOGRAM is True:\n",
        "        sample_indexes = np.random.choice(np.arange(len(train_X), dtype=int), size=10, replace=False)\n",
        "        plt.figure()\n",
        "        for (ii, jj) in enumerate(sample_indexes):\n",
        "            plt.subplot(5, 6, ii + 1)\n",
        "            plt.imshow(train_X[jj], cmap=\"gray\")\n",
        "            plt.title(\"Label: %d\" % train_Y[jj])\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}